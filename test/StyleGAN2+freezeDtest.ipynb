{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleGAN+freezeDtest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1T0UjAruNFhlvfqhI5Gomn87r-eB8fYLW",
      "authorship_tag": "ABX9TyNG1wKykazngjPdJrJ6+o/x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dakyommii/study/blob/main/test/StyleGAN2%2BfreezeDtest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVKvdvIrcQg-"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def show_tensor_images(image_tensor, num_images=16, size=(3, 64, 64)):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images,\n",
        "    size per image, and images per row, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu().clamp_(0, 1)\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=4, padding=0)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWBah2z5y7rd",
        "outputId": "35670d75-521c-49a5-c2d1-11dade81bd12"
      },
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 1.12 MiB | 1.47 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEEhL881J36X"
      },
      "source": [
        "###dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KsRrFn2cY8f"
      },
      "source": [
        "import os\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "#preprocessing \n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, n_classes=10, resolution=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # List of paths to training examples\n",
        "        self.examples = []\n",
        "        self.load_examples_from_dir(root)\n",
        "\n",
        "        # Initialize transforms\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.Resize((resolution, resolution), Image.LANCZOS),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.Lambda(lambda x: np.array(x)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ])\n",
        "\n",
        "    def load_examples_from_dir(self, abs_path):\n",
        "        '''\n",
        "        Given a folder of examples, this function returns a list of paired examples.\n",
        "        '''\n",
        "        assert os.path.isdir(abs_path)\n",
        "\n",
        "        img_suffix = '.png'\n",
        "\n",
        "        n_classes = 0\n",
        "        for root, _, files in os.walk(abs_path):\n",
        "            if n_classes == self.n_classes:\n",
        "                break\n",
        "            for f in files:\n",
        "                if f.endswith(img_suffix):\n",
        "                    self.examples.append(root + '/' + f)\n",
        "            n_classes += 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples[idx]\n",
        "        img = Image.open(example).convert('RGB')\n",
        "        return self.transforms(img)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8pQdTyed7DU"
      },
      "source": [
        "# Download Anime Face dataset to `data` folder\n",
        "# if not os.path.isdir('data/wild'):\n",
        "#     # !wget http://www.nurs.or.jp/~nagadomi/animeface-character-dataset/data/animeface-character-dataset.zip\n",
        "#     !unzip '/content/drive/MyDrive/sgData/wild.zip' -d data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L9csMzSzgvq"
      },
      "source": [
        "class FusedLeakyReLU(nn.Module):\n",
        "    def __init__(self, channel, bias=True, negative_slope=0.2, scale=2 ** 0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(channel))\n",
        "\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        self.negative_slope = negative_slope\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, input):\n",
        "        return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)\n",
        "\n",
        "\n",
        "def fused_leaky_relu(input, bias=None, negative_slope=0.2, scale=2 ** 0.5):\n",
        "    if input.device.type == \"cpu\":\n",
        "        if bias is not None:\n",
        "            rest_dim = [1] * (input.ndim - bias.ndim - 1)\n",
        "            return (\n",
        "                F.leaky_relu(\n",
        "                    input + bias.view(1, bias.shape[0], *rest_dim), negative_slope=0.2\n",
        "                )\n",
        "                * scale\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            return F.leaky_relu(input, negative_slope=0.2) * scale\n",
        "\n",
        "    else:\n",
        "        return FusedLeakyReLUFunction.apply(\n",
        "            input.contiguous(), bias, negative_slope, scale\n",
        "        )\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UguLiRnJx81"
      },
      "source": [
        "###dnnlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqkpAwAsJdor"
      },
      "source": [
        "import ctypes\n",
        "import fnmatch\n",
        "import importlib\n",
        "import inspect\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import types\n",
        "import io\n",
        "import pickle\n",
        "import re\n",
        "import requests\n",
        "import html\n",
        "import hashlib\n",
        "import glob\n",
        "import tempfile\n",
        "import urllib\n",
        "import urllib.request\n",
        "import uuid\n",
        "\n",
        "from distutils.util import strtobool\n",
        "from typing import Any, List, Tuple, Union\n",
        "\n",
        "\n",
        "# Util classes\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class EasyDict(dict):\n",
        "    \"\"\"Convenience class that behaves like a dict but allows access with the attribute syntax.\"\"\"\n",
        "\n",
        "    def __getattr__(self, name: str) -> Any:\n",
        "        try:\n",
        "            return self[name]\n",
        "        except KeyError:\n",
        "            raise AttributeError(name)\n",
        "\n",
        "    def __setattr__(self, name: str, value: Any) -> None:\n",
        "        self[name] = value\n",
        "\n",
        "    def __delattr__(self, name: str) -> None:\n",
        "        del self[name]\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    \"\"\"Redirect stderr to stdout, optionally print stdout to a file, and optionally force flushing on both stdout and the file.\"\"\"\n",
        "\n",
        "    def __init__(self, file_name: str = None, file_mode: str = \"w\", should_flush: bool = True):\n",
        "        self.file = None\n",
        "\n",
        "        if file_name is not None:\n",
        "            self.file = open(file_name, file_mode)\n",
        "\n",
        "        self.should_flush = should_flush\n",
        "        self.stdout = sys.stdout\n",
        "        self.stderr = sys.stderr\n",
        "\n",
        "        sys.stdout = self\n",
        "        sys.stderr = self\n",
        "\n",
        "    def __enter__(self) -> \"Logger\":\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n",
        "        self.close()\n",
        "\n",
        "    def write(self, text: Union[str, bytes]) -> None:\n",
        "        \"\"\"Write text to stdout (and a file) and optionally flush.\"\"\"\n",
        "        if isinstance(text, bytes):\n",
        "            text = text.decode()\n",
        "        if len(text) == 0: # workaround for a bug in VSCode debugger: sys.stdout.write(''); sys.stdout.flush() => crash\n",
        "            return\n",
        "\n",
        "        if self.file is not None:\n",
        "            self.file.write(text)\n",
        "\n",
        "        self.stdout.write(text)\n",
        "\n",
        "        if self.should_flush:\n",
        "            self.flush()\n",
        "\n",
        "    def flush(self) -> None:\n",
        "        \"\"\"Flush written text to both stdout and a file, if open.\"\"\"\n",
        "        if self.file is not None:\n",
        "            self.file.flush()\n",
        "\n",
        "        self.stdout.flush()\n",
        "\n",
        "    def close(self) -> None:\n",
        "        \"\"\"Flush, close possible files, and remove stdout/stderr mirroring.\"\"\"\n",
        "        self.flush()\n",
        "\n",
        "        # if using multiple loggers, prevent closing in wrong order\n",
        "        if sys.stdout is self:\n",
        "            sys.stdout = self.stdout\n",
        "        if sys.stderr is self:\n",
        "            sys.stderr = self.stderr\n",
        "\n",
        "        if self.file is not None:\n",
        "            self.file.close()\n",
        "            self.file = None\n",
        "\n",
        "\n",
        "# Cache directories\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "_dnnlib_cache_dir = None\n",
        "\n",
        "def set_cache_dir(path: str) -> None:\n",
        "    global _dnnlib_cache_dir\n",
        "    _dnnlib_cache_dir = path\n",
        "\n",
        "def make_cache_dir_path(*paths: str) -> str:\n",
        "    if _dnnlib_cache_dir is not None:\n",
        "        return os.path.join(_dnnlib_cache_dir, *paths)\n",
        "    if 'DNNLIB_CACHE_DIR' in os.environ:\n",
        "        return os.path.join(os.environ['DNNLIB_CACHE_DIR'], *paths)\n",
        "    if 'HOME' in os.environ:\n",
        "        return os.path.join(os.environ['HOME'], '.cache', 'dnnlib', *paths)\n",
        "    if 'USERPROFILE' in os.environ:\n",
        "        return os.path.join(os.environ['USERPROFILE'], '.cache', 'dnnlib', *paths)\n",
        "    return os.path.join(tempfile.gettempdir(), '.cache', 'dnnlib', *paths)\n",
        "\n",
        "# Small util functions\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def format_time(seconds: Union[int, float]) -> str:\n",
        "    \"\"\"Convert the seconds to human readable string with days, hours, minutes and seconds.\"\"\"\n",
        "    s = int(np.rint(seconds))\n",
        "\n",
        "    if s < 60:\n",
        "        return \"{0}s\".format(s)\n",
        "    elif s < 60 * 60:\n",
        "        return \"{0}m {1:02}s\".format(s // 60, s % 60)\n",
        "    elif s < 24 * 60 * 60:\n",
        "        return \"{0}h {1:02}m {2:02}s\".format(s // (60 * 60), (s // 60) % 60, s % 60)\n",
        "    else:\n",
        "        return \"{0}d {1:02}h {2:02}m\".format(s // (24 * 60 * 60), (s // (60 * 60)) % 24, (s // 60) % 60)\n",
        "\n",
        "\n",
        "def ask_yes_no(question: str) -> bool:\n",
        "    \"\"\"Ask the user the question until the user inputs a valid answer.\"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            print(\"{0} [y/n]\".format(question))\n",
        "            return strtobool(input().lower())\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "\n",
        "def tuple_product(t: Tuple) -> Any:\n",
        "    \"\"\"Calculate the product of the tuple elements.\"\"\"\n",
        "    result = 1\n",
        "\n",
        "    for v in t:\n",
        "        result *= v\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "_str_to_ctype = {\n",
        "    \"uint8\": ctypes.c_ubyte,\n",
        "    \"uint16\": ctypes.c_uint16,\n",
        "    \"uint32\": ctypes.c_uint32,\n",
        "    \"uint64\": ctypes.c_uint64,\n",
        "    \"int8\": ctypes.c_byte,\n",
        "    \"int16\": ctypes.c_int16,\n",
        "    \"int32\": ctypes.c_int32,\n",
        "    \"int64\": ctypes.c_int64,\n",
        "    \"float32\": ctypes.c_float,\n",
        "    \"float64\": ctypes.c_double\n",
        "}\n",
        "\n",
        "\n",
        "def get_dtype_and_ctype(type_obj: Any) -> Tuple[np.dtype, Any]:\n",
        "    \"\"\"Given a type name string (or an object having a __name__ attribute), return matching Numpy and ctypes types that have the same size in bytes.\"\"\"\n",
        "    type_str = None\n",
        "\n",
        "    if isinstance(type_obj, str):\n",
        "        type_str = type_obj\n",
        "    elif hasattr(type_obj, \"__name__\"):\n",
        "        type_str = type_obj.__name__\n",
        "    elif hasattr(type_obj, \"name\"):\n",
        "        type_str = type_obj.name\n",
        "    else:\n",
        "        raise RuntimeError(\"Cannot infer type name from input\")\n",
        "\n",
        "    assert type_str in _str_to_ctype.keys()\n",
        "\n",
        "    my_dtype = np.dtype(type_str)\n",
        "    my_ctype = _str_to_ctype[type_str]\n",
        "\n",
        "    assert my_dtype.itemsize == ctypes.sizeof(my_ctype)\n",
        "\n",
        "    return my_dtype, my_ctype\n",
        "\n",
        "\n",
        "def is_pickleable(obj: Any) -> bool:\n",
        "    try:\n",
        "        with io.BytesIO() as stream:\n",
        "            pickle.dump(obj, stream)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "\n",
        "# Functionality to import modules/objects by name, and call functions by name\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "def get_module_from_obj_name(obj_name: str) -> Tuple[types.ModuleType, str]:\n",
        "    \"\"\"Searches for the underlying module behind the name to some python object.\n",
        "    Returns the module and the object name (original name with module part removed).\"\"\"\n",
        "\n",
        "    # allow convenience shorthands, substitute them by full names\n",
        "    obj_name = re.sub(\"^np.\", \"numpy.\", obj_name)\n",
        "    obj_name = re.sub(\"^tf.\", \"tensorflow.\", obj_name)\n",
        "\n",
        "    # list alternatives for (module_name, local_obj_name)\n",
        "    parts = obj_name.split(\".\")\n",
        "    name_pairs = [(\".\".join(parts[:i]), \".\".join(parts[i:])) for i in range(len(parts), 0, -1)]\n",
        "\n",
        "    # try each alternative in turn\n",
        "    for module_name, local_obj_name in name_pairs:\n",
        "        try:\n",
        "            module = importlib.import_module(module_name) # may raise ImportError\n",
        "            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n",
        "            return module, local_obj_name\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # maybe some of the modules themselves contain errors?\n",
        "    for module_name, _local_obj_name in name_pairs:\n",
        "        try:\n",
        "            importlib.import_module(module_name) # may raise ImportError\n",
        "        except ImportError:\n",
        "            if not str(sys.exc_info()[1]).startswith(\"No module named '\" + module_name + \"'\"):\n",
        "                raise\n",
        "\n",
        "    # maybe the requested attribute is missing?\n",
        "    for module_name, local_obj_name in name_pairs:\n",
        "        try:\n",
        "            module = importlib.import_module(module_name) # may raise ImportError\n",
        "            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n",
        "        except ImportError:\n",
        "            pass\n",
        "\n",
        "    # we are out of luck, but we have no idea why\n",
        "    raise ImportError(obj_name)\n",
        "\n",
        "\n",
        "def get_obj_from_module(module: types.ModuleType, obj_name: str) -> Any:\n",
        "    \"\"\"Traverses the object name and returns the last (rightmost) python object.\"\"\"\n",
        "    if obj_name == '':\n",
        "        return module\n",
        "    obj = module\n",
        "    for part in obj_name.split(\".\"):\n",
        "        obj = getattr(obj, part)\n",
        "    return obj\n",
        "\n",
        "\n",
        "def get_obj_by_name(name: str) -> Any:\n",
        "    \"\"\"Finds the python object with the given name.\"\"\"\n",
        "    module, obj_name = get_module_from_obj_name(name)\n",
        "    return get_obj_from_module(module, obj_name)\n",
        "\n",
        "\n",
        "def call_func_by_name(*args, func_name: str = None, **kwargs) -> Any:\n",
        "    \"\"\"Finds the python object with the given name and calls it as a function.\"\"\"\n",
        "    assert func_name is not None\n",
        "    func_obj = get_obj_by_name(func_name)\n",
        "    assert callable(func_obj)\n",
        "    return func_obj(*args, **kwargs)\n",
        "\n",
        "\n",
        "def construct_class_by_name(*args, class_name: str = None, **kwargs) -> Any:\n",
        "    \"\"\"Finds the python class with the given name and constructs it with the given arguments.\"\"\"\n",
        "    return call_func_by_name(*args, func_name=class_name, **kwargs)\n",
        "\n",
        "\n",
        "def get_module_dir_by_obj_name(obj_name: str) -> str:\n",
        "    \"\"\"Get the directory path of the module containing the given object name.\"\"\"\n",
        "    module, _ = get_module_from_obj_name(obj_name)\n",
        "    return os.path.dirname(inspect.getfile(module))\n",
        "\n",
        "\n",
        "def is_top_level_function(obj: Any) -> bool:\n",
        "    \"\"\"Determine whether the given object is a top-level function, i.e., defined at module scope using 'def'.\"\"\"\n",
        "    return callable(obj) and obj.__name__ in sys.modules[obj.__module__].__dict__\n",
        "\n",
        "\n",
        "def get_top_level_function_name(obj: Any) -> str:\n",
        "    \"\"\"Return the fully-qualified name of a top-level function.\"\"\"\n",
        "    assert is_top_level_function(obj)\n",
        "    module = obj.__module__\n",
        "    if module == '__main__':\n",
        "        module = os.path.splitext(os.path.basename(sys.modules[module].__file__))[0]\n",
        "    return module + \".\" + obj.__name__\n",
        "\n",
        "\n",
        "# File system helpers\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "def list_dir_recursively_with_ignore(dir_path: str, ignores: List[str] = None, add_base_to_relative: bool = False) -> List[Tuple[str, str]]:\n",
        "    \"\"\"List all files recursively in a given directory while ignoring given file and directory names.\n",
        "    Returns list of tuples containing both absolute and relative paths.\"\"\"\n",
        "    assert os.path.isdir(dir_path)\n",
        "    base_name = os.path.basename(os.path.normpath(dir_path))\n",
        "\n",
        "    if ignores is None:\n",
        "        ignores = []\n",
        "\n",
        "    result = []\n",
        "\n",
        "    for root, dirs, files in os.walk(dir_path, topdown=True):\n",
        "        for ignore_ in ignores:\n",
        "            dirs_to_remove = [d for d in dirs if fnmatch.fnmatch(d, ignore_)]\n",
        "\n",
        "            # dirs need to be edited in-place\n",
        "            for d in dirs_to_remove:\n",
        "                dirs.remove(d)\n",
        "\n",
        "            files = [f for f in files if not fnmatch.fnmatch(f, ignore_)]\n",
        "\n",
        "        absolute_paths = [os.path.join(root, f) for f in files]\n",
        "        relative_paths = [os.path.relpath(p, dir_path) for p in absolute_paths]\n",
        "\n",
        "        if add_base_to_relative:\n",
        "            relative_paths = [os.path.join(base_name, p) for p in relative_paths]\n",
        "\n",
        "        assert len(absolute_paths) == len(relative_paths)\n",
        "        result += zip(absolute_paths, relative_paths)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def copy_files_and_create_dirs(files: List[Tuple[str, str]]) -> None:\n",
        "    \"\"\"Takes in a list of tuples of (src, dst) paths and copies files.\n",
        "    Will create all necessary directories.\"\"\"\n",
        "    for file in files:\n",
        "        target_dir_name = os.path.dirname(file[1])\n",
        "\n",
        "        # will create all intermediate-level directories\n",
        "        if not os.path.exists(target_dir_name):\n",
        "            os.makedirs(target_dir_name)\n",
        "\n",
        "        shutil.copyfile(file[0], file[1])\n",
        "\n",
        "\n",
        "# URL helpers\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "def is_url(obj: Any, allow_file_urls: bool = False) -> bool:\n",
        "    \"\"\"Determine whether the given object is a valid URL string.\"\"\"\n",
        "    if not isinstance(obj, str) or not \"://\" in obj:\n",
        "        return False\n",
        "    if allow_file_urls and obj.startswith('file://'):\n",
        "        return True\n",
        "    try:\n",
        "        res = requests.compat.urlparse(obj)\n",
        "        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n",
        "            return False\n",
        "        res = requests.compat.urlparse(requests.compat.urljoin(obj, \"/\"))\n",
        "        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n",
        "            return False\n",
        "    except:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def open_url(url: str, cache_dir: str = None, num_attempts: int = 10, verbose: bool = True, return_filename: bool = False, cache: bool = True) -> Any:\n",
        "    \"\"\"Download the given URL and return a binary-mode file object to access the data.\"\"\"\n",
        "    assert num_attempts >= 1\n",
        "    assert not (return_filename and (not cache))\n",
        "\n",
        "    # Doesn't look like an URL scheme so interpret it as a local filename.\n",
        "    if not re.match('^[a-z]+://', url):\n",
        "        return url if return_filename else open(url, \"rb\")\n",
        "\n",
        "    # Handle file URLs.  This code handles unusual file:// patterns that\n",
        "    # arise on Windows:\n",
        "    #\n",
        "    # file:///c:/foo.txt\n",
        "    #\n",
        "    # which would translate to a local '/c:/foo.txt' filename that's\n",
        "    # invalid.  Drop the forward slash for such pathnames.\n",
        "    #\n",
        "    # If you touch this code path, you should test it on both Linux and\n",
        "    # Windows.\n",
        "    #\n",
        "    # Some internet resources suggest using urllib.request.url2pathname() but\n",
        "    # but that converts forward slashes to backslashes and this causes\n",
        "    # its own set of problems.\n",
        "    if url.startswith('file://'):\n",
        "        filename = urllib.parse.urlparse(url).path\n",
        "        if re.match(r'^/[a-zA-Z]:', filename):\n",
        "            filename = filename[1:]\n",
        "        return filename if return_filename else open(filename, \"rb\")\n",
        "\n",
        "    assert is_url(url)\n",
        "\n",
        "    # Lookup from cache.\n",
        "    if cache_dir is None:\n",
        "        cache_dir = make_cache_dir_path('downloads')\n",
        "\n",
        "    url_md5 = hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n",
        "    if cache:\n",
        "        cache_files = glob.glob(os.path.join(cache_dir, url_md5 + \"_*\"))\n",
        "        if len(cache_files) == 1:\n",
        "            filename = cache_files[0]\n",
        "            return filename if return_filename else open(filename, \"rb\")\n",
        "\n",
        "    # Download.\n",
        "    url_name = None\n",
        "    url_data = None\n",
        "    with requests.Session() as session:\n",
        "        if verbose:\n",
        "            print(\"Downloading %s ...\" % url, end=\"\", flush=True)\n",
        "        for attempts_left in reversed(range(num_attempts)):\n",
        "            try:\n",
        "                with session.get(url) as res:\n",
        "                    res.raise_for_status()\n",
        "                    if len(res.content) == 0:\n",
        "                        raise IOError(\"No data received\")\n",
        "\n",
        "                    if len(res.content) < 8192:\n",
        "                        content_str = res.content.decode(\"utf-8\")\n",
        "                        if \"download_warning\" in res.headers.get(\"Set-Cookie\", \"\"):\n",
        "                            links = [html.unescape(link) for link in content_str.split('\"') if \"export=download\" in link]\n",
        "                            if len(links) == 1:\n",
        "                                url = requests.compat.urljoin(url, links[0])\n",
        "                                raise IOError(\"Google Drive virus checker nag\")\n",
        "                        if \"Google Drive - Quota exceeded\" in content_str:\n",
        "                            raise IOError(\"Google Drive download quota exceeded -- please try again later\")\n",
        "\n",
        "                    match = re.search(r'filename=\"([^\"]*)\"', res.headers.get(\"Content-Disposition\", \"\"))\n",
        "                    url_name = match[1] if match else url\n",
        "                    url_data = res.content\n",
        "                    if verbose:\n",
        "                        print(\" done\")\n",
        "                    break\n",
        "            except KeyboardInterrupt:\n",
        "                raise\n",
        "            except:\n",
        "                if not attempts_left:\n",
        "                    if verbose:\n",
        "                        print(\" failed\")\n",
        "                    raise\n",
        "                if verbose:\n",
        "                    print(\".\", end=\"\", flush=True)\n",
        "\n",
        "    # Save to cache.\n",
        "    if cache:\n",
        "        safe_name = re.sub(r\"[^0-9a-zA-Z-._]\", \"_\", url_name)\n",
        "        cache_file = os.path.join(cache_dir, url_md5 + \"_\" + safe_name)\n",
        "        temp_file = os.path.join(cache_dir, \"tmp_\" + uuid.uuid4().hex + \"_\" + url_md5 + \"_\" + safe_name)\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        with open(temp_file, \"wb\") as f:\n",
        "            f.write(url_data)\n",
        "        os.replace(temp_file, cache_file) # atomic\n",
        "        if return_filename:\n",
        "            return cache_file\n",
        "\n",
        "    # Return data as file object.\n",
        "    assert not return_filename\n",
        "    return io.BytesIO(url_data)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ECMT5roJtZy"
      },
      "source": [
        "###misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEDI_szLJPt6"
      },
      "source": [
        "\n",
        "import re\n",
        "import contextlib\n",
        "import numpy as np\n",
        "import torch\n",
        "import warnings\n",
        "# import dnnlib\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Cached construction of constant tensors. Avoids CPU=>GPU copy when the\n",
        "# same constant is used multiple times.\n",
        "\n",
        "_constant_cache = dict()\n",
        "\n",
        "def constant(value, shape=None, dtype=None, device=None, memory_format=None):\n",
        "    value = np.asarray(value)\n",
        "    if shape is not None:\n",
        "        shape = tuple(shape)\n",
        "    if dtype is None:\n",
        "        dtype = torch.get_default_dtype()\n",
        "    if device is None:\n",
        "        device = torch.device('cpu')\n",
        "    if memory_format is None:\n",
        "        memory_format = torch.contiguous_format\n",
        "\n",
        "    key = (value.shape, value.dtype, value.tobytes(), shape, dtype, device, memory_format)\n",
        "    tensor = _constant_cache.get(key, None)\n",
        "    if tensor is None:\n",
        "        tensor = torch.as_tensor(value.copy(), dtype=dtype, device=device)\n",
        "        if shape is not None:\n",
        "            tensor, _ = torch.broadcast_tensors(tensor, torch.empty(shape))\n",
        "        tensor = tensor.contiguous(memory_format=memory_format)\n",
        "        _constant_cache[key] = tensor\n",
        "    return tensor\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Replace NaN/Inf with specified numerical values.\n",
        "\n",
        "try:\n",
        "    nan_to_num = torch.nan_to_num # 1.8.0a0\n",
        "except AttributeError:\n",
        "    def nan_to_num(input, nan=0.0, posinf=None, neginf=None, *, out=None): # pylint: disable=redefined-builtin\n",
        "        assert isinstance(input, torch.Tensor)\n",
        "        if posinf is None:\n",
        "            posinf = torch.finfo(input.dtype).max\n",
        "        if neginf is None:\n",
        "            neginf = torch.finfo(input.dtype).min\n",
        "        assert nan == 0\n",
        "        return torch.clamp(input.unsqueeze(0).nansum(0), min=neginf, max=posinf, out=out)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Symbolic assert.\n",
        "\n",
        "try:\n",
        "    symbolic_assert = torch._assert # 1.8.0a0 # pylint: disable=protected-access\n",
        "except AttributeError:\n",
        "    symbolic_assert = torch.Assert # 1.7.0\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Context manager to suppress known warnings in torch.jit.trace().\n",
        "\n",
        "class suppress_tracer_warnings(warnings.catch_warnings):\n",
        "    def __enter__(self):\n",
        "        super().__enter__()\n",
        "        warnings.simplefilter('ignore', category=torch.jit.TracerWarning)\n",
        "        return self\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Assert that the shape of a tensor matches the given list of integers.\n",
        "# None indicates that the size of a dimension is allowed to vary.\n",
        "# Performs symbolic assertion when used in torch.jit.trace().\n",
        "\n",
        "def assert_shape(tensor, ref_shape):\n",
        "    if tensor.ndim != len(ref_shape):\n",
        "        raise AssertionError(f'Wrong number of dimensions: got {tensor.ndim}, expected {len(ref_shape)}')\n",
        "    for idx, (size, ref_size) in enumerate(zip(tensor.shape, ref_shape)):\n",
        "        if ref_size is None:\n",
        "            pass\n",
        "        elif isinstance(ref_size, torch.Tensor):\n",
        "            with suppress_tracer_warnings(): # as_tensor results are registered as constants\n",
        "                symbolic_assert(torch.equal(torch.as_tensor(size), ref_size), f'Wrong size for dimension {idx}')\n",
        "        elif isinstance(size, torch.Tensor):\n",
        "            with suppress_tracer_warnings(): # as_tensor results are registered as constants\n",
        "                symbolic_assert(torch.equal(size, torch.as_tensor(ref_size)), f'Wrong size for dimension {idx}: expected {ref_size}')\n",
        "        elif size != ref_size:\n",
        "            raise AssertionError(f'Wrong size for dimension {idx}: got {size}, expected {ref_size}')\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Function decorator that calls torch.autograd.profiler.record_function().\n",
        "\n",
        "def profiled_function(fn):\n",
        "    def decorator(*args, **kwargs):\n",
        "        with torch.autograd.profiler.record_function(fn.__name__):\n",
        "            return fn(*args, **kwargs)\n",
        "    decorator.__name__ = fn.__name__\n",
        "    return decorator\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Sampler for torch.utils.data.DataLoader that loops over the dataset\n",
        "# indefinitely, shuffling items as it goes.\n",
        "\n",
        "class InfiniteSampler(torch.utils.data.Sampler):\n",
        "    def __init__(self, dataset, rank=0, num_replicas=1, shuffle=True, seed=0, window_size=0.5):\n",
        "        assert len(dataset) > 0\n",
        "        assert num_replicas > 0\n",
        "        assert 0 <= rank < num_replicas\n",
        "        assert 0 <= window_size <= 1\n",
        "        super().__init__(dataset)\n",
        "        self.dataset = dataset\n",
        "        self.rank = rank\n",
        "        self.num_replicas = num_replicas\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        order = np.arange(len(self.dataset))\n",
        "        rnd = None\n",
        "        window = 0\n",
        "        if self.shuffle:\n",
        "            rnd = np.random.RandomState(self.seed)\n",
        "            rnd.shuffle(order)\n",
        "            window = int(np.rint(order.size * self.window_size))\n",
        "\n",
        "        idx = 0\n",
        "        while True:\n",
        "            i = idx % order.size\n",
        "            if idx % self.num_replicas == self.rank:\n",
        "                yield order[i]\n",
        "            if window >= 2:\n",
        "                j = (i - rnd.randint(window)) % order.size\n",
        "                order[i], order[j] = order[j], order[i]\n",
        "            idx += 1\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Utilities for operating with torch.nn.Module parameters and buffers.\n",
        "\n",
        "def params_and_buffers(module):\n",
        "    assert isinstance(module, torch.nn.Module)\n",
        "    return list(module.parameters()) + list(module.buffers())\n",
        "\n",
        "def named_params_and_buffers(module):\n",
        "    assert isinstance(module, torch.nn.Module)\n",
        "    return list(module.named_parameters()) + list(module.named_buffers())\n",
        "\n",
        "def copy_params_and_buffers(src_module, dst_module, require_all=False):\n",
        "    assert isinstance(src_module, torch.nn.Module)\n",
        "    assert isinstance(dst_module, torch.nn.Module)\n",
        "    src_tensors = {name: tensor for name, tensor in named_params_and_buffers(src_module)}\n",
        "    for name, tensor in named_params_and_buffers(dst_module):\n",
        "        assert (name in src_tensors) or (not require_all)\n",
        "        if name in src_tensors:\n",
        "            tensor.copy_(src_tensors[name].detach()).requires_grad_(tensor.requires_grad)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Context manager for easily enabling/disabling DistributedDataParallel\n",
        "# synchronization.\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def ddp_sync(module, sync):\n",
        "    assert isinstance(module, torch.nn.Module)\n",
        "    if sync or not isinstance(module, torch.nn.parallel.DistributedDataParallel):\n",
        "        yield\n",
        "    else:\n",
        "        with module.no_sync():\n",
        "            yield\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Check DistributedDataParallel consistency across processes.\n",
        "\n",
        "def check_ddp_consistency(module, ignore_regex=None):\n",
        "    assert isinstance(module, torch.nn.Module)\n",
        "    for name, tensor in named_params_and_buffers(module):\n",
        "        fullname = type(module).__name__ + '.' + name\n",
        "        if ignore_regex is not None and re.fullmatch(ignore_regex, fullname):\n",
        "            continue\n",
        "        tensor = tensor.detach()\n",
        "        other = tensor.clone()\n",
        "        torch.distributed.broadcast(tensor=other, src=0)\n",
        "        assert (nan_to_num(tensor) == nan_to_num(other)).all(), fullname\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Print summary table of module hierarchy.\n",
        "\n",
        "def print_module_summary(module, inputs, max_nesting=3, skip_redundant=True):\n",
        "    assert isinstance(module, torch.nn.Module)\n",
        "    assert not isinstance(module, torch.jit.ScriptModule)\n",
        "    assert isinstance(inputs, (tuple, list))\n",
        "\n",
        "    # Register hooks.\n",
        "    entries = []\n",
        "    nesting = [0]\n",
        "    def pre_hook(_mod, _inputs):\n",
        "        nesting[0] += 1\n",
        "    def post_hook(mod, _inputs, outputs):\n",
        "        nesting[0] -= 1\n",
        "        if nesting[0] <= max_nesting:\n",
        "            outputs = list(outputs) if isinstance(outputs, (tuple, list)) else [outputs]\n",
        "            outputs = [t for t in outputs if isinstance(t, torch.Tensor)]\n",
        "            entries.append(dnnlib.EasyDict(mod=mod, outputs=outputs))\n",
        "    hooks = [mod.register_forward_pre_hook(pre_hook) for mod in module.modules()]\n",
        "    hooks += [mod.register_forward_hook(post_hook) for mod in module.modules()]\n",
        "\n",
        "    # Run module.\n",
        "    outputs = module(*inputs)\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    # Identify unique outputs, parameters, and buffers.\n",
        "    tensors_seen = set()\n",
        "    for e in entries:\n",
        "        e.unique_params = [t for t in e.mod.parameters() if id(t) not in tensors_seen]\n",
        "        e.unique_buffers = [t for t in e.mod.buffers() if id(t) not in tensors_seen]\n",
        "        e.unique_outputs = [t for t in e.outputs if id(t) not in tensors_seen]\n",
        "        tensors_seen |= {id(t) for t in e.unique_params + e.unique_buffers + e.unique_outputs}\n",
        "\n",
        "    # Filter out redundant entries.\n",
        "    if skip_redundant:\n",
        "        entries = [e for e in entries if len(e.unique_params) or len(e.unique_buffers) or len(e.unique_outputs)]\n",
        "\n",
        "    # Construct table.\n",
        "    rows = [[type(module).__name__, 'Parameters', 'Buffers', 'Output shape', 'Datatype']]\n",
        "    rows += [['---'] * len(rows[0])]\n",
        "    param_total = 0\n",
        "    buffer_total = 0\n",
        "    submodule_names = {mod: name for name, mod in module.named_modules()}\n",
        "    for e in entries:\n",
        "        name = '<top-level>' if e.mod is module else submodule_names[e.mod]\n",
        "        param_size = sum(t.numel() for t in e.unique_params)\n",
        "        buffer_size = sum(t.numel() for t in e.unique_buffers)\n",
        "        output_shapes = [str(list(e.outputs[0].shape)) for t in e.outputs]\n",
        "        output_dtypes = [str(t.dtype).split('.')[-1] for t in e.outputs]\n",
        "        rows += [[\n",
        "            name + (':0' if len(e.outputs) >= 2 else ''),\n",
        "            str(param_size) if param_size else '-',\n",
        "            str(buffer_size) if buffer_size else '-',\n",
        "            (output_shapes + ['-'])[0],\n",
        "            (output_dtypes + ['-'])[0],\n",
        "        ]]\n",
        "        for idx in range(1, len(e.outputs)):\n",
        "            rows += [[name + f':{idx}', '-', '-', output_shapes[idx], output_dtypes[idx]]]\n",
        "        param_total += param_size\n",
        "        buffer_total += buffer_size\n",
        "    rows += [['---'] * len(rows[0])]\n",
        "    rows += [['Total', str(param_total), str(buffer_total), '-', '-']]\n",
        "\n",
        "    # Print table.\n",
        "    widths = [max(len(cell) for cell in column) for column in zip(*rows)]\n",
        "    print()\n",
        "    for row in rows:\n",
        "        print('  '.join(cell + ' ' * (width - len(cell)) for cell, width in zip(row, widths)))\n",
        "    print()\n",
        "    return outputs"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfK79c82PVLl"
      },
      "source": [
        "###persistence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbHvh0wCPXGT"
      },
      "source": [
        "import sys\n",
        "import pickle\n",
        "import io\n",
        "import inspect\n",
        "import copy\n",
        "import uuid\n",
        "import types\n",
        "# import dnnlib\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "_version            = 6         # internal version number\n",
        "_decorators         = set()     # {decorator_class, ...}\n",
        "_import_hooks       = []        # [hook_function, ...]\n",
        "_module_to_src_dict = dict()    # {module: src, ...}\n",
        "_src_to_module_dict = dict()    # {src: module, ...}\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def persistent_class(orig_class):\n",
        "    r\"\"\"Class decorator that extends a given class to save its source code\n",
        "    when pickled.\n",
        "    Example:\n",
        "        from torch_utils import persistence\n",
        "        @persistence.persistent_class\n",
        "        class MyNetwork(torch.nn.Module):\n",
        "            def __init__(self, num_inputs, num_outputs):\n",
        "                super().__init__()\n",
        "                self.fc = MyLayer(num_inputs, num_outputs)\n",
        "                ...\n",
        "        @persistence.persistent_class\n",
        "        class MyLayer(torch.nn.Module):\n",
        "            ...\n",
        "    When pickled, any instance of `MyNetwork` and `MyLayer` will save its\n",
        "    source code alongside other internal state (e.g., parameters, buffers,\n",
        "    and submodules). This way, any previously exported pickle will remain\n",
        "    usable even if the class definitions have been modified or are no\n",
        "    longer available.\n",
        "    The decorator saves the source code of the entire Python module\n",
        "    containing the decorated class. It does *not* save the source code of\n",
        "    any imported modules. Thus, the imported modules must be available\n",
        "    during unpickling, also including `torch_utils.persistence` itself.\n",
        "    It is ok to call functions defined in the same module from the\n",
        "    decorated class. However, if the decorated class depends on other\n",
        "    classes defined in the same module, they must be decorated as well.\n",
        "    This is illustrated in the above example in the case of `MyLayer`.\n",
        "    It is also possible to employ the decorator just-in-time before\n",
        "    calling the constructor. For example:\n",
        "        cls = MyLayer\n",
        "        if want_to_make_it_persistent:\n",
        "            cls = persistence.persistent_class(cls)\n",
        "        layer = cls(num_inputs, num_outputs)\n",
        "    As an additional feature, the decorator also keeps track of the\n",
        "    arguments that were used to construct each instance of the decorated\n",
        "    class. The arguments can be queried via `obj.init_args` and\n",
        "    `obj.init_kwargs`, and they are automatically pickled alongside other\n",
        "    object state. A typical use case is to first unpickle a previous\n",
        "    instance of a persistent class, and then upgrade it to use the latest\n",
        "    version of the source code:\n",
        "        with open('old_pickle.pkl', 'rb') as f:\n",
        "            old_net = pickle.load(f)\n",
        "        new_net = MyNetwork(*old_obj.init_args, **old_obj.init_kwargs)\n",
        "        misc.copy_params_and_buffers(old_net, new_net, require_all=True)\n",
        "    \"\"\"\n",
        "    assert isinstance(orig_class, type)\n",
        "    if is_persistent(orig_class):\n",
        "        return orig_class\n",
        "\n",
        "    assert orig_class.__module__ in sys.modules\n",
        "    orig_module = sys.modules[orig_class.__module__]\n",
        "    orig_module_src = _module_to_src(orig_module)\n",
        "\n",
        "    class Decorator(orig_class):\n",
        "        _orig_module_src = orig_module_src\n",
        "        _orig_class_name = orig_class.__name__\n",
        "\n",
        "        def __init__(self, *args, **kwargs):\n",
        "            super().__init__(*args, **kwargs)\n",
        "            self._init_args = copy.deepcopy(args)\n",
        "            self._init_kwargs = copy.deepcopy(kwargs)\n",
        "            assert orig_class.__name__ in orig_module.__dict__\n",
        "            _check_pickleable(self.__reduce__())\n",
        "\n",
        "        @property\n",
        "        def init_args(self):\n",
        "            return copy.deepcopy(self._init_args)\n",
        "\n",
        "        @property\n",
        "        def init_kwargs(self):\n",
        "            return dnnlib.EasyDict(copy.deepcopy(self._init_kwargs))\n",
        "\n",
        "        def __reduce__(self):\n",
        "            fields = list(super().__reduce__())\n",
        "            fields += [None] * max(3 - len(fields), 0)\n",
        "            if fields[0] is not _reconstruct_persistent_obj:\n",
        "                meta = dict(type='class', version=_version, module_src=self._orig_module_src, class_name=self._orig_class_name, state=fields[2])\n",
        "                fields[0] = _reconstruct_persistent_obj # reconstruct func\n",
        "                fields[1] = (meta,) # reconstruct args\n",
        "                fields[2] = None # state dict\n",
        "            return tuple(fields)\n",
        "\n",
        "    Decorator.__name__ = orig_class.__name__\n",
        "    _decorators.add(Decorator)\n",
        "    return Decorator\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def is_persistent(obj):\n",
        "    r\"\"\"Test whether the given object or class is persistent, i.e.,\n",
        "    whether it will save its source code when pickled.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if obj in _decorators:\n",
        "            return True\n",
        "    except TypeError:\n",
        "        pass\n",
        "    return type(obj) in _decorators # pylint: disable=unidiomatic-typecheck\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def import_hook(hook):\n",
        "    r\"\"\"Register an import hook that is called whenever a persistent object\n",
        "    is being unpickled. A typical use case is to patch the pickled source\n",
        "    code to avoid errors and inconsistencies when the API of some imported\n",
        "    module has changed.\n",
        "    The hook should have the following signature:\n",
        "        hook(meta) -> modified meta\n",
        "    `meta` is an instance of `dnnlib.EasyDict` with the following fields:\n",
        "        type:       Type of the persistent object, e.g. `'class'`.\n",
        "        version:    Internal version number of `torch_utils.persistence`.\n",
        "        module_src  Original source code of the Python module.\n",
        "        class_name: Class name in the original Python module.\n",
        "        state:      Internal state of the object.\n",
        "    Example:\n",
        "        @persistence.import_hook\n",
        "        def wreck_my_network(meta):\n",
        "            if meta.class_name == 'MyNetwork':\n",
        "                print('MyNetwork is being imported. I will wreck it!')\n",
        "                meta.module_src = meta.module_src.replace(\"True\", \"False\")\n",
        "            return meta\n",
        "    \"\"\"\n",
        "    assert callable(hook)\n",
        "    _import_hooks.append(hook)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def _reconstruct_persistent_obj(meta):\n",
        "    r\"\"\"Hook that is called internally by the `pickle` module to unpickle\n",
        "    a persistent object.\n",
        "    \"\"\"\n",
        "    meta = dnnlib.EasyDict(meta)\n",
        "    meta.state = dnnlib.EasyDict(meta.state)\n",
        "    for hook in _import_hooks:\n",
        "        meta = hook(meta)\n",
        "        assert meta is not None\n",
        "\n",
        "    assert meta.version == _version\n",
        "    module = _src_to_module(meta.module_src)\n",
        "\n",
        "    assert meta.type == 'class'\n",
        "    orig_class = module.__dict__[meta.class_name]\n",
        "    decorator_class = persistent_class(orig_class)\n",
        "    obj = decorator_class.__new__(decorator_class)\n",
        "\n",
        "    setstate = getattr(obj, '__setstate__', None)\n",
        "    if callable(setstate):\n",
        "        setstate(meta.state) # pylint: disable=not-callable\n",
        "    else:\n",
        "        obj.__dict__.update(meta.state)\n",
        "    return obj\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def _module_to_src(module):\n",
        "    r\"\"\"Query the source code of a given Python module.\n",
        "    \"\"\"\n",
        "    src = _module_to_src_dict.get(module, None)\n",
        "    if src is None:\n",
        "        src = inspect.getsource(module)\n",
        "        _module_to_src_dict[module] = src\n",
        "        _src_to_module_dict[src] = module\n",
        "    return src\n",
        "\n",
        "def _src_to_module(src):\n",
        "    r\"\"\"Get or create a Python module for the given source code.\n",
        "    \"\"\"\n",
        "    module = _src_to_module_dict.get(src, None)\n",
        "    if module is None:\n",
        "        module_name = \"_imported_module_\" + uuid.uuid4().hex\n",
        "        module = types.ModuleType(module_name)\n",
        "        sys.modules[module_name] = module\n",
        "        _module_to_src_dict[module] = src\n",
        "        _src_to_module_dict[src] = module\n",
        "        exec(src, module.__dict__) # pylint: disable=exec-used\n",
        "    return module\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def _check_pickleable(obj):\n",
        "    r\"\"\"Check that the given object is pickleable, raising an exception if\n",
        "    it is not. This function is expected to be considerably more efficient\n",
        "    than actually pickling the object.\n",
        "    \"\"\"\n",
        "    def recurse(obj):\n",
        "        if isinstance(obj, (list, tuple, set)):\n",
        "            return [recurse(x) for x in obj]\n",
        "        if isinstance(obj, dict):\n",
        "            return [[recurse(x), recurse(y)] for x, y in obj.items()]\n",
        "        if isinstance(obj, (str, int, float, bool, bytes, bytearray)):\n",
        "            return None # Python primitive types are pickleable.\n",
        "        if f'{type(obj).__module__}.{type(obj).__name__}' in ['numpy.ndarray', 'torch.Tensor']:\n",
        "            return None # NumPy arrays and PyTorch tensors are pickleable.\n",
        "        if is_persistent(obj):\n",
        "            return None # Persistent objects are pickleable, by virtue of the constructor check.\n",
        "        return obj\n",
        "    with io.BytesIO() as f:\n",
        "        pickle.dump(recurse(obj), f)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WELnwAS0Pgi_"
      },
      "source": [
        "###conv2d_resample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-W1dTb-PuHQ"
      },
      "source": [
        "import torch\n",
        "\n",
        "from .. import misc\n",
        "from . import conv2d_gradfix\n",
        "from . import upfirdn2d\n",
        "from .upfirdn2d import _parse_padding\n",
        "from .upfirdn2d import _get_filter_size\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def _get_weight_shape(w):\n",
        "    with misc.suppress_tracer_warnings(): # this value will be treated as a constant\n",
        "        shape = [int(sz) for sz in w.shape]\n",
        "    misc.assert_shape(w, shape)\n",
        "    return shape\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def _conv2d_wrapper(x, w, stride=1, padding=0, groups=1, transpose=False, flip_weight=True):\n",
        "    \"\"\"Wrapper for the underlying `conv2d()` and `conv_transpose2d()` implementations.\n",
        "    \"\"\"\n",
        "    out_channels, in_channels_per_group, kh, kw = _get_weight_shape(w)\n",
        "\n",
        "    # Flip weight if requested.\n",
        "    if not flip_weight: # conv2d() actually performs correlation (flip_weight=True) not convolution (flip_weight=False).\n",
        "        w = w.flip([2, 3])\n",
        "\n",
        "    # Workaround performance pitfall in cuDNN 8.0.5, triggered when using\n",
        "    # 1x1 kernel + memory_format=channels_last + less than 64 channels.\n",
        "    if kw == 1 and kh == 1 and stride == 1 and padding in [0, [0, 0], (0, 0)] and not transpose:\n",
        "        if x.stride()[1] == 1 and min(out_channels, in_channels_per_group) < 64:\n",
        "            if out_channels <= 4 and groups == 1:\n",
        "                in_shape = x.shape\n",
        "                x = w.squeeze(3).squeeze(2) @ x.reshape([in_shape[0], in_channels_per_group, -1])\n",
        "                x = x.reshape([in_shape[0], out_channels, in_shape[2], in_shape[3]])\n",
        "            else:\n",
        "                x = x.to(memory_format=torch.contiguous_format)\n",
        "                w = w.to(memory_format=torch.contiguous_format)\n",
        "                x = conv2d_gradfix.conv2d(x, w, groups=groups)\n",
        "            return x.to(memory_format=torch.channels_last)\n",
        "\n",
        "    # Otherwise => execute using conv2d_gradfix.\n",
        "    op = conv2d_gradfix.conv_transpose2d if transpose else conv2d_gradfix.conv2d\n",
        "    return op(x, w, stride=stride, padding=padding, groups=groups)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@misc.profiled_function\n",
        "def conv2d_resample(x, w, f=None, up=1, down=1, padding=0, groups=1, flip_weight=True, flip_filter=False):\n",
        "    r\"\"\"2D convolution with optional up/downsampling.\n",
        "    Padding is performed only once at the beginning, not between the operations.\n",
        "    Args:\n",
        "        x:              Input tensor of shape\n",
        "                        `[batch_size, in_channels, in_height, in_width]`.\n",
        "        w:              Weight tensor of shape\n",
        "                        `[out_channels, in_channels//groups, kernel_height, kernel_width]`.\n",
        "        f:              Low-pass filter for up/downsampling. Must be prepared beforehand by\n",
        "                        calling upfirdn2d.setup_filter(). None = identity (default).\n",
        "        up:             Integer upsampling factor (default: 1).\n",
        "        down:           Integer downsampling factor (default: 1).\n",
        "        padding:        Padding with respect to the upsampled image. Can be a single number\n",
        "                        or a list/tuple `[x, y]` or `[x_before, x_after, y_before, y_after]`\n",
        "                        (default: 0).\n",
        "        groups:         Split input channels into N groups (default: 1).\n",
        "        flip_weight:    False = convolution, True = correlation (default: True).\n",
        "        flip_filter:    False = convolution, True = correlation (default: False).\n",
        "    Returns:\n",
        "        Tensor of the shape `[batch_size, num_channels, out_height, out_width]`.\n",
        "    \"\"\"\n",
        "    # Validate arguments.\n",
        "    assert isinstance(x, torch.Tensor) and (x.ndim == 4)\n",
        "    assert isinstance(w, torch.Tensor) and (w.ndim == 4) and (w.dtype == x.dtype)\n",
        "    assert f is None or (isinstance(f, torch.Tensor) and f.ndim in [1, 2] and f.dtype == torch.float32)\n",
        "    assert isinstance(up, int) and (up >= 1)\n",
        "    assert isinstance(down, int) and (down >= 1)\n",
        "    assert isinstance(groups, int) and (groups >= 1)\n",
        "    out_channels, in_channels_per_group, kh, kw = _get_weight_shape(w)\n",
        "    fw, fh = _get_filter_size(f)\n",
        "    px0, px1, py0, py1 = _parse_padding(padding)\n",
        "\n",
        "    # Adjust padding to account for up/downsampling.\n",
        "    if up > 1:\n",
        "        px0 += (fw + up - 1) // 2\n",
        "        px1 += (fw - up) // 2\n",
        "        py0 += (fh + up - 1) // 2\n",
        "        py1 += (fh - up) // 2\n",
        "    if down > 1:\n",
        "        px0 += (fw - down + 1) // 2\n",
        "        px1 += (fw - down) // 2\n",
        "        py0 += (fh - down + 1) // 2\n",
        "        py1 += (fh - down) // 2\n",
        "\n",
        "    # Fast path: 1x1 convolution with downsampling only => downsample first, then convolve.\n",
        "    if kw == 1 and kh == 1 and (down > 1 and up == 1):\n",
        "        x = upfirdn2d.upfirdn2d(x=x, f=f, down=down, padding=[px0,px1,py0,py1], flip_filter=flip_filter)\n",
        "        x = _conv2d_wrapper(x=x, w=w, groups=groups, flip_weight=flip_weight)\n",
        "        return x\n",
        "\n",
        "    # Fast path: 1x1 convolution with upsampling only => convolve first, then upsample.\n",
        "    if kw == 1 and kh == 1 and (up > 1 and down == 1):\n",
        "        x = _conv2d_wrapper(x=x, w=w, groups=groups, flip_weight=flip_weight)\n",
        "        x = upfirdn2d.upfirdn2d(x=x, f=f, up=up, padding=[px0,px1,py0,py1], gain=up**2, flip_filter=flip_filter)\n",
        "        return x\n",
        "\n",
        "    # Fast path: downsampling only => use strided convolution.\n",
        "    if down > 1 and up == 1:\n",
        "        x = upfirdn2d.upfirdn2d(x=x, f=f, padding=[px0,px1,py0,py1], flip_filter=flip_filter)\n",
        "        x = _conv2d_wrapper(x=x, w=w, stride=down, groups=groups, flip_weight=flip_weight)\n",
        "        return x\n",
        "\n",
        "    # Fast path: upsampling with optional downsampling => use transpose strided convolution.\n",
        "    if up > 1:\n",
        "        if groups == 1:\n",
        "            w = w.transpose(0, 1)\n",
        "        else:\n",
        "            w = w.reshape(groups, out_channels // groups, in_channels_per_group, kh, kw)\n",
        "            w = w.transpose(1, 2)\n",
        "            w = w.reshape(groups * in_channels_per_group, out_channels // groups, kh, kw)\n",
        "        px0 -= kw - 1\n",
        "        px1 -= kw - up\n",
        "        py0 -= kh - 1\n",
        "        py1 -= kh - up\n",
        "        pxt = max(min(-px0, -px1), 0)\n",
        "        pyt = max(min(-py0, -py1), 0)\n",
        "        x = _conv2d_wrapper(x=x, w=w, stride=up, padding=[pyt,pxt], groups=groups, transpose=True, flip_weight=(not flip_weight))\n",
        "        x = upfirdn2d.upfirdn2d(x=x, f=f, padding=[px0+pxt,px1+pxt,py0+pyt,py1+pyt], gain=up**2, flip_filter=flip_filter)\n",
        "        if down > 1:\n",
        "            x = upfirdn2d.upfirdn2d(x=x, f=f, down=down, flip_filter=flip_filter)\n",
        "        return x\n",
        "\n",
        "    # Fast path: no up/downsampling, padding supported by the underlying implementation => use plain conv2d.\n",
        "    if up == 1 and down == 1:\n",
        "        if px0 == px1 and py0 == py1 and px0 >= 0 and py0 >= 0:\n",
        "            return _conv2d_wrapper(x=x, w=w, padding=[py0,px0], groups=groups, flip_weight=flip_weight)\n",
        "\n",
        "    # Fallback: Generic reference implementation.\n",
        "    x = upfirdn2d.upfirdn2d(x=x, f=(f if up > 1 else None), up=up, padding=[px0,px1,py0,py1], gain=up**2, flip_filter=flip_filter)\n",
        "    x = _conv2d_wrapper(x=x, w=w, groups=groups, flip_weight=flip_weight)\n",
        "    if down > 1:\n",
        "        x = upfirdn2d.upfirdn2d(x=x, f=f, down=down, flip_filter=flip_filter)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYkAdGQjKM80"
      },
      "source": [
        "###model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "AIRDuCnchD1A",
        "outputId": "1128377a-cc37-4c76-bbd4-95f292740fb6"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_utils import misc\n",
        "from torch_utils import persistence\n",
        "from torch_utils.ops import conv2d_resample\n",
        "from torch_utils.ops import upfirdn2d\n",
        "from torch_utils.ops import bias_act\n",
        "from torch_utils.ops import fma\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@misc.profiled_function\n",
        "def normalize_2nd_moment(x, dim=1, eps=1e-8):\n",
        "    return x * (x.square().mean(dim=dim, keepdim=True) + eps).rsqrt()\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@misc.profiled_function\n",
        "def modulated_conv2d(\n",
        "    x,                          # Input tensor of shape [batch_size, in_channels, in_height, in_width].\n",
        "    weight,                     # Weight tensor of shape [out_channels, in_channels, kernel_height, kernel_width].\n",
        "    styles,                     # Modulation coefficients of shape [batch_size, in_channels].\n",
        "    noise           = None,     # Optional noise tensor to add to the output activations.\n",
        "    up              = 1,        # Integer upsampling factor.\n",
        "    down            = 1,        # Integer downsampling factor.\n",
        "    padding         = 0,        # Padding with respect to the upsampled image.\n",
        "    resample_filter = None,     # Low-pass filter to apply when resampling activations. Must be prepared beforehand by calling upfirdn2d.setup_filter().\n",
        "    demodulate      = True,     # Apply weight demodulation?\n",
        "    flip_weight     = True,     # False = convolution, True = correlation (matches torch.nn.functional.conv2d).\n",
        "    fused_modconv   = True,     # Perform modulation, convolution, and demodulation as a single fused operation?\n",
        "):\n",
        "    batch_size = x.shape[0]\n",
        "    out_channels, in_channels, kh, kw = weight.shape\n",
        "    misc.assert_shape(weight, [out_channels, in_channels, kh, kw]) # [OIkk]\n",
        "    misc.assert_shape(x, [batch_size, in_channels, None, None]) # [NIHW]\n",
        "    misc.assert_shape(styles, [batch_size, in_channels]) # [NI]\n",
        "\n",
        "    # Pre-normalize inputs to avoid FP16 overflow.\n",
        "    if x.dtype == torch.float16 and demodulate:\n",
        "        weight = weight * (1 / np.sqrt(in_channels * kh * kw) / weight.norm(float('inf'), dim=[1,2,3], keepdim=True)) # max_Ikk\n",
        "        styles = styles / styles.norm(float('inf'), dim=1, keepdim=True) # max_I\n",
        "\n",
        "    # Calculate per-sample weights and demodulation coefficients.\n",
        "    w = None\n",
        "    dcoefs = None\n",
        "    if demodulate or fused_modconv:\n",
        "        w = weight.unsqueeze(0) # [NOIkk]\n",
        "        w = w * styles.reshape(batch_size, 1, -1, 1, 1) # [NOIkk]\n",
        "    if demodulate:\n",
        "        dcoefs = (w.square().sum(dim=[2,3,4]) + 1e-8).rsqrt() # [NO]\n",
        "    if demodulate and fused_modconv:\n",
        "        w = w * dcoefs.reshape(batch_size, -1, 1, 1, 1) # [NOIkk]\n",
        "\n",
        "    # Execute by scaling the activations before and after the convolution.\n",
        "    if not fused_modconv:\n",
        "        x = x * styles.to(x.dtype).reshape(batch_size, -1, 1, 1)\n",
        "        x = conv2d_resample.conv2d_resample(x=x, w=weight.to(x.dtype), f=resample_filter, up=up, down=down, padding=padding, flip_weight=flip_weight)\n",
        "        if demodulate and noise is not None:\n",
        "            x = fma.fma(x, dcoefs.to(x.dtype).reshape(batch_size, -1, 1, 1), noise.to(x.dtype))\n",
        "        elif demodulate:\n",
        "            x = x * dcoefs.to(x.dtype).reshape(batch_size, -1, 1, 1)\n",
        "        elif noise is not None:\n",
        "            x = x.add_(noise.to(x.dtype))\n",
        "        return x\n",
        "\n",
        "    # Execute as one fused op using grouped convolution.\n",
        "    with misc.suppress_tracer_warnings(): # this value will be treated as a constant\n",
        "        batch_size = int(batch_size)\n",
        "    misc.assert_shape(x, [batch_size, in_channels, None, None])\n",
        "    x = x.reshape(1, -1, *x.shape[2:])\n",
        "    w = w.reshape(-1, in_channels, kh, kw)\n",
        "    x = conv2d_resample.conv2d_resample(x=x, w=w.to(x.dtype), f=resample_filter, up=up, down=down, padding=padding, groups=batch_size, flip_weight=flip_weight)\n",
        "    x = x.reshape(batch_size, -1, *x.shape[2:])\n",
        "    if noise is not None:\n",
        "        x = x.add_(noise)\n",
        "    return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class FullyConnectedLayer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_features,                # Number of input features.\n",
        "        out_features,               # Number of output features.\n",
        "        bias            = True,     # Apply additive bias before the activation function?\n",
        "        activation      = 'linear', # Activation function: 'relu', 'lrelu', etc.\n",
        "        lr_multiplier   = 1,        # Learning rate multiplier.\n",
        "        bias_init       = 0,        # Initial value for the additive bias.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        self.weight = torch.nn.Parameter(torch.randn([out_features, in_features]) / lr_multiplier)\n",
        "        self.bias = torch.nn.Parameter(torch.full([out_features], np.float32(bias_init))) if bias else None\n",
        "        self.weight_gain = lr_multiplier / np.sqrt(in_features)\n",
        "        self.bias_gain = lr_multiplier\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = self.weight.to(x.dtype) * self.weight_gain\n",
        "        b = self.bias\n",
        "        if b is not None:\n",
        "            b = b.to(x.dtype)\n",
        "            if self.bias_gain != 1:\n",
        "                b = b * self.bias_gain\n",
        "\n",
        "        if self.activation == 'linear' and b is not None:\n",
        "            x = torch.addmm(b.unsqueeze(0), x, w.t())\n",
        "        else:\n",
        "            x = x.matmul(w.t())\n",
        "            x = bias_act.bias_act(x, b, act=self.activation)\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class Conv2dLayer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels,                    # Number of input channels.\n",
        "        out_channels,                   # Number of output channels.\n",
        "        kernel_size,                    # Width and height of the convolution kernel.\n",
        "        bias            = True,         # Apply additive bias before the activation function?\n",
        "        activation      = 'linear',     # Activation function: 'relu', 'lrelu', etc.\n",
        "        up              = 1,            # Integer upsampling factor.\n",
        "        down            = 1,            # Integer downsampling factor.\n",
        "        resample_filter = [1,3,3,1],    # Low-pass filter to apply when resampling activations.\n",
        "        conv_clamp      = None,         # Clamp the output to +-X, None = disable clamping.\n",
        "        channels_last   = False,        # Expect the input to have memory_format=channels_last?\n",
        "        trainable       = True,         # Update the weights of this layer during training?\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        self.up = up\n",
        "        self.down = down\n",
        "        self.conv_clamp = conv_clamp\n",
        "        self.register_buffer('resample_filter', upfirdn2d.setup_filter(resample_filter))\n",
        "        self.padding = kernel_size // 2\n",
        "        self.weight_gain = 1 / np.sqrt(in_channels * (kernel_size ** 2))\n",
        "        self.act_gain = bias_act.activation_funcs[activation].def_gain\n",
        "\n",
        "        memory_format = torch.channels_last if channels_last else torch.contiguous_format\n",
        "        weight = torch.randn([out_channels, in_channels, kernel_size, kernel_size]).to(memory_format=memory_format)\n",
        "        bias = torch.zeros([out_channels]) if bias else None\n",
        "        if trainable:\n",
        "            self.weight = torch.nn.Parameter(weight)\n",
        "            self.bias = torch.nn.Parameter(bias) if bias is not None else None\n",
        "        else:\n",
        "            self.register_buffer('weight', weight)\n",
        "            if bias is not None:\n",
        "                self.register_buffer('bias', bias)\n",
        "            else:\n",
        "                self.bias = None\n",
        "\n",
        "    def forward(self, x, gain=1):\n",
        "        w = self.weight * self.weight_gain\n",
        "        b = self.bias.to(x.dtype) if self.bias is not None else None\n",
        "        flip_weight = (self.up == 1) # slightly faster\n",
        "        x = conv2d_resample.conv2d_resample(x=x, w=w.to(x.dtype), f=self.resample_filter, up=self.up, down=self.down, padding=self.padding, flip_weight=flip_weight)\n",
        "\n",
        "        act_gain = self.act_gain * gain\n",
        "        act_clamp = self.conv_clamp * gain if self.conv_clamp is not None else None\n",
        "        x = bias_act.bias_act(x, b, act=self.activation, gain=act_gain, clamp=act_clamp)\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class MappingNetwork(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        z_dim,                      # Input latent (Z) dimensionality, 0 = no latent.\n",
        "        c_dim,                      # Conditioning label (C) dimensionality, 0 = no label.\n",
        "        w_dim,                      # Intermediate latent (W) dimensionality.\n",
        "        num_ws,                     # Number of intermediate latents to output, None = do not broadcast.\n",
        "        num_layers      = 8,        # Number of mapping layers.\n",
        "        embed_features  = None,     # Label embedding dimensionality, None = same as w_dim.\n",
        "        layer_features  = None,     # Number of intermediate features in the mapping layers, None = same as w_dim.\n",
        "        activation      = 'lrelu',  # Activation function: 'relu', 'lrelu', etc.\n",
        "        lr_multiplier   = 0.01,     # Learning rate multiplier for the mapping layers.\n",
        "        w_avg_beta      = 0.995,    # Decay for tracking the moving average of W during training, None = do not track.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.c_dim = c_dim\n",
        "        self.w_dim = w_dim\n",
        "        self.num_ws = num_ws\n",
        "        self.num_layers = num_layers\n",
        "        self.w_avg_beta = w_avg_beta\n",
        "\n",
        "        if embed_features is None:\n",
        "            embed_features = w_dim\n",
        "        if c_dim == 0:\n",
        "            embed_features = 0\n",
        "        if layer_features is None:\n",
        "            layer_features = w_dim\n",
        "        features_list = [z_dim + embed_features] + [layer_features] * (num_layers - 1) + [w_dim]\n",
        "\n",
        "        if c_dim > 0:\n",
        "            self.embed = FullyConnectedLayer(c_dim, embed_features)\n",
        "        for idx in range(num_layers):\n",
        "            in_features = features_list[idx]\n",
        "            out_features = features_list[idx + 1]\n",
        "            layer = FullyConnectedLayer(in_features, out_features, activation=activation, lr_multiplier=lr_multiplier)\n",
        "            setattr(self, f'fc{idx}', layer)\n",
        "\n",
        "        if num_ws is not None and w_avg_beta is not None:\n",
        "            self.register_buffer('w_avg', torch.zeros([w_dim]))\n",
        "\n",
        "    def forward(self, z, c, truncation_psi=1, truncation_cutoff=None, skip_w_avg_update=False):\n",
        "        # Embed, normalize, and concat inputs.\n",
        "        x = None\n",
        "        with torch.autograd.profiler.record_function('input'):\n",
        "            if self.z_dim > 0:\n",
        "                misc.assert_shape(z, [None, self.z_dim])\n",
        "                x = normalize_2nd_moment(z.to(torch.float32))\n",
        "            if self.c_dim > 0:\n",
        "                misc.assert_shape(c, [None, self.c_dim])\n",
        "                y = normalize_2nd_moment(self.embed(c.to(torch.float32)))\n",
        "                x = torch.cat([x, y], dim=1) if x is not None else y\n",
        "\n",
        "        # Main layers.\n",
        "        for idx in range(self.num_layers):\n",
        "            layer = getattr(self, f'fc{idx}')\n",
        "            x = layer(x)\n",
        "\n",
        "        # Update moving average of W.\n",
        "        if self.w_avg_beta is not None and self.training and not skip_w_avg_update:\n",
        "            with torch.autograd.profiler.record_function('update_w_avg'):\n",
        "                self.w_avg.copy_(x.detach().mean(dim=0).lerp(self.w_avg, self.w_avg_beta))\n",
        "\n",
        "        # Broadcast.\n",
        "        if self.num_ws is not None:\n",
        "            with torch.autograd.profiler.record_function('broadcast'):\n",
        "                x = x.unsqueeze(1).repeat([1, self.num_ws, 1])\n",
        "\n",
        "        # Apply truncation.\n",
        "        if truncation_psi != 1:\n",
        "            with torch.autograd.profiler.record_function('truncate'):\n",
        "                assert self.w_avg_beta is not None\n",
        "                if self.num_ws is None or truncation_cutoff is None:\n",
        "                    x = self.w_avg.lerp(x, truncation_psi)\n",
        "                else:\n",
        "                    x[:, :truncation_cutoff] = self.w_avg.lerp(x[:, :truncation_cutoff], truncation_psi)\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class SynthesisLayer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels,                    # Number of input channels.\n",
        "        out_channels,                   # Number of output channels.\n",
        "        w_dim,                          # Intermediate latent (W) dimensionality.\n",
        "        resolution,                     # Resolution of this layer.\n",
        "        kernel_size     = 3,            # Convolution kernel size.\n",
        "        up              = 1,            # Integer upsampling factor.\n",
        "        use_noise       = True,         # Enable noise input?\n",
        "        activation      = 'lrelu',      # Activation function: 'relu', 'lrelu', etc.\n",
        "        resample_filter = [1,3,3,1],    # Low-pass filter to apply when resampling activations.\n",
        "        conv_clamp      = None,         # Clamp the output of convolution layers to +-X, None = disable clamping.\n",
        "        channels_last   = False,        # Use channels_last format for the weights?\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.resolution = resolution\n",
        "        self.up = up\n",
        "        self.use_noise = use_noise\n",
        "        self.activation = activation\n",
        "        self.conv_clamp = conv_clamp\n",
        "        self.register_buffer('resample_filter', upfirdn2d.setup_filter(resample_filter))\n",
        "        self.padding = kernel_size // 2\n",
        "        self.act_gain = bias_act.activation_funcs[activation].def_gain\n",
        "\n",
        "        self.affine = FullyConnectedLayer(w_dim, in_channels, bias_init=1)\n",
        "        memory_format = torch.channels_last if channels_last else torch.contiguous_format\n",
        "        self.weight = torch.nn.Parameter(torch.randn([out_channels, in_channels, kernel_size, kernel_size]).to(memory_format=memory_format))\n",
        "        if use_noise:\n",
        "            self.register_buffer('noise_const', torch.randn([resolution, resolution]))\n",
        "            self.noise_strength = torch.nn.Parameter(torch.zeros([]))\n",
        "        self.bias = torch.nn.Parameter(torch.zeros([out_channels]))\n",
        "\n",
        "    def forward(self, x, w, noise_mode='random', fused_modconv=True, gain=1):\n",
        "        assert noise_mode in ['random', 'const', 'none']\n",
        "        in_resolution = self.resolution // self.up\n",
        "        misc.assert_shape(x, [None, self.weight.shape[1], in_resolution, in_resolution])\n",
        "        styles = self.affine(w)\n",
        "\n",
        "        noise = None\n",
        "        if self.use_noise and noise_mode == 'random':\n",
        "            noise = torch.randn([x.shape[0], 1, self.resolution, self.resolution], device=x.device) * self.noise_strength\n",
        "        if self.use_noise and noise_mode == 'const':\n",
        "            noise = self.noise_const * self.noise_strength\n",
        "\n",
        "        flip_weight = (self.up == 1) # slightly faster\n",
        "        x = modulated_conv2d(x=x, weight=self.weight, styles=styles, noise=noise, up=self.up,\n",
        "            padding=self.padding, resample_filter=self.resample_filter, flip_weight=flip_weight, fused_modconv=fused_modconv)\n",
        "\n",
        "        act_gain = self.act_gain * gain\n",
        "        act_clamp = self.conv_clamp * gain if self.conv_clamp is not None else None\n",
        "        x = bias_act.bias_act(x, self.bias.to(x.dtype), act=self.activation, gain=act_gain, clamp=act_clamp)\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class ToRGBLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, w_dim, kernel_size=1, conv_clamp=None, channels_last=False):\n",
        "        super().__init__()\n",
        "        self.conv_clamp = conv_clamp\n",
        "        self.affine = FullyConnectedLayer(w_dim, in_channels, bias_init=1)\n",
        "        memory_format = torch.channels_last if channels_last else torch.contiguous_format\n",
        "        self.weight = torch.nn.Parameter(torch.randn([out_channels, in_channels, kernel_size, kernel_size]).to(memory_format=memory_format))\n",
        "        self.bias = torch.nn.Parameter(torch.zeros([out_channels]))\n",
        "        self.weight_gain = 1 / np.sqrt(in_channels * (kernel_size ** 2))\n",
        "\n",
        "    def forward(self, x, w, fused_modconv=True):\n",
        "        styles = self.affine(w) * self.weight_gain\n",
        "        x = modulated_conv2d(x=x, weight=self.weight, styles=styles, demodulate=False, fused_modconv=fused_modconv)\n",
        "        x = bias_act.bias_act(x, self.bias.to(x.dtype), clamp=self.conv_clamp)\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class SynthesisBlock(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels,                        # Number of input channels, 0 = first block.\n",
        "        out_channels,                       # Number of output channels.\n",
        "        w_dim,                              # Intermediate latent (W) dimensionality.\n",
        "        resolution,                         # Resolution of this block.\n",
        "        img_channels,                       # Number of output color channels.\n",
        "        is_last,                            # Is this the last block?\n",
        "        architecture        = 'skip',       # Architecture: 'orig', 'skip', 'resnet'.\n",
        "        resample_filter     = [1,3,3,1],    # Low-pass filter to apply when resampling activations.\n",
        "        conv_clamp          = None,         # Clamp the output of convolution layers to +-X, None = disable clamping.\n",
        "        use_fp16            = False,        # Use FP16 for this block?\n",
        "        fp16_channels_last  = False,        # Use channels-last memory format with FP16?\n",
        "        **layer_kwargs,                     # Arguments for SynthesisLayer.\n",
        "    ):\n",
        "        assert architecture in ['orig', 'skip', 'resnet']\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.w_dim = w_dim\n",
        "        self.resolution = resolution\n",
        "        self.img_channels = img_channels\n",
        "        self.is_last = is_last\n",
        "        self.architecture = architecture\n",
        "        self.use_fp16 = use_fp16\n",
        "        self.channels_last = (use_fp16 and fp16_channels_last)\n",
        "        self.register_buffer('resample_filter', upfirdn2d.setup_filter(resample_filter))\n",
        "        self.num_conv = 0\n",
        "        self.num_torgb = 0\n",
        "\n",
        "        if in_channels == 0:\n",
        "            self.const = torch.nn.Parameter(torch.randn([out_channels, resolution, resolution]))\n",
        "\n",
        "        if in_channels != 0:\n",
        "            self.conv0 = SynthesisLayer(in_channels, out_channels, w_dim=w_dim, resolution=resolution, up=2,\n",
        "                resample_filter=resample_filter, conv_clamp=conv_clamp, channels_last=self.channels_last, **layer_kwargs)\n",
        "            self.num_conv += 1\n",
        "\n",
        "        self.conv1 = SynthesisLayer(out_channels, out_channels, w_dim=w_dim, resolution=resolution,\n",
        "            conv_clamp=conv_clamp, channels_last=self.channels_last, **layer_kwargs)\n",
        "        self.num_conv += 1\n",
        "\n",
        "        if is_last or architecture == 'skip':\n",
        "            self.torgb = ToRGBLayer(out_channels, img_channels, w_dim=w_dim,\n",
        "                conv_clamp=conv_clamp, channels_last=self.channels_last)\n",
        "            self.num_torgb += 1\n",
        "\n",
        "        if in_channels != 0 and architecture == 'resnet':\n",
        "            self.skip = Conv2dLayer(in_channels, out_channels, kernel_size=1, bias=False, up=2,\n",
        "                resample_filter=resample_filter, channels_last=self.channels_last)\n",
        "\n",
        "    def forward(self, x, img, ws, force_fp32=False, fused_modconv=None, **layer_kwargs):\n",
        "        misc.assert_shape(ws, [None, self.num_conv + self.num_torgb, self.w_dim])\n",
        "        w_iter = iter(ws.unbind(dim=1))\n",
        "        dtype = torch.float16 if self.use_fp16 and not force_fp32 else torch.float32\n",
        "        memory_format = torch.channels_last if self.channels_last and not force_fp32 else torch.contiguous_format\n",
        "        if fused_modconv is None:\n",
        "            with misc.suppress_tracer_warnings(): # this value will be treated as a constant\n",
        "                fused_modconv = (not self.training) and (dtype == torch.float32 or int(x.shape[0]) == 1)\n",
        "\n",
        "        # Input.\n",
        "        if self.in_channels == 0:\n",
        "            x = self.const.to(dtype=dtype, memory_format=memory_format)\n",
        "            x = x.unsqueeze(0).repeat([ws.shape[0], 1, 1, 1])\n",
        "        else:\n",
        "            misc.assert_shape(x, [None, self.in_channels, self.resolution // 2, self.resolution // 2])\n",
        "            x = x.to(dtype=dtype, memory_format=memory_format)\n",
        "\n",
        "        # Main layers.\n",
        "        if self.in_channels == 0:\n",
        "            x = self.conv1(x, next(w_iter), fused_modconv=fused_modconv, **layer_kwargs)\n",
        "        elif self.architecture == 'resnet':\n",
        "            y = self.skip(x, gain=np.sqrt(0.5))\n",
        "            x = self.conv0(x, next(w_iter), fused_modconv=fused_modconv, **layer_kwargs)\n",
        "            x = self.conv1(x, next(w_iter), fused_modconv=fused_modconv, gain=np.sqrt(0.5), **layer_kwargs)\n",
        "            x = y.add_(x)\n",
        "        else:\n",
        "            x = self.conv0(x, next(w_iter), fused_modconv=fused_modconv, **layer_kwargs)\n",
        "            x = self.conv1(x, next(w_iter), fused_modconv=fused_modconv, **layer_kwargs)\n",
        "\n",
        "        # ToRGB.\n",
        "        if img is not None:\n",
        "            misc.assert_shape(img, [None, self.img_channels, self.resolution // 2, self.resolution // 2])\n",
        "            img = upfirdn2d.upsample2d(img, self.resample_filter)\n",
        "        if self.is_last or self.architecture == 'skip':\n",
        "            y = self.torgb(x, next(w_iter), fused_modconv=fused_modconv)\n",
        "            y = y.to(dtype=torch.float32, memory_format=torch.contiguous_format)\n",
        "            img = img.add_(y) if img is not None else y\n",
        "\n",
        "        assert x.dtype == dtype\n",
        "        assert img is None or img.dtype == torch.float32\n",
        "        return x, img\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class SynthesisNetwork(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        w_dim,                      # Intermediate latent (W) dimensionality.\n",
        "        img_resolution,             # Output image resolution.\n",
        "        img_channels,               # Number of color channels.\n",
        "        channel_base    = 32768,    # Overall multiplier for the number of channels.\n",
        "        channel_max     = 512,      # Maximum number of channels in any layer.\n",
        "        num_fp16_res    = 0,        # Use FP16 for the N highest resolutions.\n",
        "        **block_kwargs,             # Arguments for SynthesisBlock.\n",
        "    ):\n",
        "        assert img_resolution >= 4 and img_resolution & (img_resolution - 1) == 0\n",
        "        super().__init__()\n",
        "        self.w_dim = w_dim\n",
        "        self.img_resolution = img_resolution\n",
        "        self.img_resolution_log2 = int(np.log2(img_resolution))\n",
        "        self.img_channels = img_channels\n",
        "        self.block_resolutions = [2 ** i for i in range(2, self.img_resolution_log2 + 1)]\n",
        "        channels_dict = {res: min(channel_base // res, channel_max) for res in self.block_resolutions}\n",
        "        fp16_resolution = max(2 ** (self.img_resolution_log2 + 1 - num_fp16_res), 8)\n",
        "\n",
        "        self.num_ws = 0\n",
        "        for res in self.block_resolutions:\n",
        "            in_channels = channels_dict[res // 2] if res > 4 else 0\n",
        "            out_channels = channels_dict[res]\n",
        "            use_fp16 = (res >= fp16_resolution)\n",
        "            is_last = (res == self.img_resolution)\n",
        "            block = SynthesisBlock(in_channels, out_channels, w_dim=w_dim, resolution=res,\n",
        "                img_channels=img_channels, is_last=is_last, use_fp16=use_fp16, **block_kwargs)\n",
        "            self.num_ws += block.num_conv\n",
        "            if is_last:\n",
        "                self.num_ws += block.num_torgb\n",
        "            setattr(self, f'b{res}', block)\n",
        "\n",
        "    def forward(self, ws, **block_kwargs):\n",
        "        block_ws = []\n",
        "        with torch.autograd.profiler.record_function('split_ws'):\n",
        "            misc.assert_shape(ws, [None, self.num_ws, self.w_dim])\n",
        "            ws = ws.to(torch.float32)\n",
        "            w_idx = 0\n",
        "            for res in self.block_resolutions:\n",
        "                block = getattr(self, f'b{res}')\n",
        "                block_ws.append(ws.narrow(1, w_idx, block.num_conv + block.num_torgb))\n",
        "                w_idx += block.num_conv\n",
        "\n",
        "        x = img = None\n",
        "        for res, cur_ws in zip(self.block_resolutions, block_ws):\n",
        "            block = getattr(self, f'b{res}')\n",
        "            x, img = block(x, img, cur_ws, **block_kwargs)\n",
        "        return img\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        z_dim,                      # Input latent (Z) dimensionality.\n",
        "        c_dim,                      # Conditioning label (C) dimensionality.\n",
        "        w_dim,                      # Intermediate latent (W) dimensionality.\n",
        "        img_resolution,             # Output resolution.\n",
        "        img_channels,               # Number of output color channels.\n",
        "        mapping_kwargs      = {},   # Arguments for MappingNetwork.\n",
        "        synthesis_kwargs    = {},   # Arguments for SynthesisNetwork.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.c_dim = c_dim\n",
        "        self.w_dim = w_dim\n",
        "        self.img_resolution = img_resolution\n",
        "        self.img_channels = img_channels\n",
        "        self.synthesis = SynthesisNetwork(w_dim=w_dim, img_resolution=img_resolution, img_channels=img_channels, **synthesis_kwargs)\n",
        "        self.num_ws = self.synthesis.num_ws\n",
        "        self.mapping = MappingNetwork(z_dim=z_dim, c_dim=c_dim, w_dim=w_dim, num_ws=self.num_ws, **mapping_kwargs)\n",
        "\n",
        "    def forward(self, z, c, truncation_psi=1, truncation_cutoff=None, **synthesis_kwargs):\n",
        "        ws = self.mapping(z, c, truncation_psi=truncation_psi, truncation_cutoff=truncation_cutoff)\n",
        "        img = self.synthesis(ws, **synthesis_kwargs)\n",
        "        return img\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class DiscriminatorBlock(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels,                        # Number of input channels, 0 = first block.\n",
        "        tmp_channels,                       # Number of intermediate channels.\n",
        "        out_channels,                       # Number of output channels.\n",
        "        resolution,                         # Resolution of this block.\n",
        "        img_channels,                       # Number of input color channels.\n",
        "        first_layer_idx,                    # Index of the first layer.\n",
        "        architecture        = 'resnet',     # Architecture: 'orig', 'skip', 'resnet'.\n",
        "        activation          = 'lrelu',      # Activation function: 'relu', 'lrelu', etc.\n",
        "        resample_filter     = [1,3,3,1],    # Low-pass filter to apply when resampling activations.\n",
        "        conv_clamp          = None,         # Clamp the output of convolution layers to +-X, None = disable clamping.\n",
        "        use_fp16            = False,        # Use FP16 for this block?\n",
        "        fp16_channels_last  = False,        # Use channels-last memory format with FP16?\n",
        "        freeze_layers       = 0,            # Freeze-D: Number of layers to freeze.\n",
        "    ):\n",
        "        assert in_channels in [0, tmp_channels]\n",
        "        assert architecture in ['orig', 'skip', 'resnet']\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.resolution = resolution\n",
        "        self.img_channels = img_channels\n",
        "        self.first_layer_idx = first_layer_idx\n",
        "        self.architecture = architecture\n",
        "        self.use_fp16 = use_fp16\n",
        "        self.channels_last = (use_fp16 and fp16_channels_last)\n",
        "        self.register_buffer('resample_filter', upfirdn2d.setup_filter(resample_filter))\n",
        "\n",
        "        self.num_layers = 0\n",
        "        def trainable_gen():\n",
        "            while True:\n",
        "                layer_idx = self.first_layer_idx + self.num_layers\n",
        "                trainable = (layer_idx >= freeze_layers)\n",
        "                self.num_layers += 1\n",
        "                yield trainable\n",
        "        trainable_iter = trainable_gen()\n",
        "\n",
        "        if in_channels == 0 or architecture == 'skip':\n",
        "            self.fromrgb = Conv2dLayer(img_channels, tmp_channels, kernel_size=1, activation=activation,\n",
        "                trainable=next(trainable_iter), conv_clamp=conv_clamp, channels_last=self.channels_last)\n",
        "\n",
        "        self.conv0 = Conv2dLayer(tmp_channels, tmp_channels, kernel_size=3, activation=activation,\n",
        "            trainable=next(trainable_iter), conv_clamp=conv_clamp, channels_last=self.channels_last)\n",
        "\n",
        "        self.conv1 = Conv2dLayer(tmp_channels, out_channels, kernel_size=3, activation=activation, down=2,\n",
        "            trainable=next(trainable_iter), resample_filter=resample_filter, conv_clamp=conv_clamp, channels_last=self.channels_last)\n",
        "\n",
        "        if architecture == 'resnet':\n",
        "            self.skip = Conv2dLayer(tmp_channels, out_channels, kernel_size=1, bias=False, down=2,\n",
        "                trainable=next(trainable_iter), resample_filter=resample_filter, channels_last=self.channels_last)\n",
        "\n",
        "    def forward(self, x, img, force_fp32=False):\n",
        "        dtype = torch.float16 if self.use_fp16 and not force_fp32 else torch.float32\n",
        "        memory_format = torch.channels_last if self.channels_last and not force_fp32 else torch.contiguous_format\n",
        "\n",
        "        # Input.\n",
        "        if x is not None:\n",
        "            misc.assert_shape(x, [None, self.in_channels, self.resolution, self.resolution])\n",
        "            x = x.to(dtype=dtype, memory_format=memory_format)\n",
        "\n",
        "        # FromRGB.\n",
        "        if self.in_channels == 0 or self.architecture == 'skip':\n",
        "            misc.assert_shape(img, [None, self.img_channels, self.resolution, self.resolution])\n",
        "            img = img.to(dtype=dtype, memory_format=memory_format)\n",
        "            y = self.fromrgb(img)\n",
        "            x = x + y if x is not None else y\n",
        "            img = upfirdn2d.downsample2d(img, self.resample_filter) if self.architecture == 'skip' else None\n",
        "\n",
        "        # Main layers.\n",
        "        if self.architecture == 'resnet':\n",
        "            y = self.skip(x, gain=np.sqrt(0.5))\n",
        "            x = self.conv0(x)\n",
        "            x = self.conv1(x, gain=np.sqrt(0.5))\n",
        "            x = y.add_(x)\n",
        "        else:\n",
        "            x = self.conv0(x)\n",
        "            x = self.conv1(x)\n",
        "\n",
        "        assert x.dtype == dtype\n",
        "        return x, img\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class MinibatchStdLayer(torch.nn.Module):\n",
        "    def __init__(self, group_size, num_channels=1):\n",
        "        super().__init__()\n",
        "        self.group_size = group_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        with misc.suppress_tracer_warnings(): # as_tensor results are registered as constants\n",
        "            G = torch.min(torch.as_tensor(self.group_size), torch.as_tensor(N)) if self.group_size is not None else N\n",
        "        F = self.num_channels\n",
        "        c = C // F\n",
        "\n",
        "        y = x.reshape(G, -1, F, c, H, W)    # [GnFcHW] Split minibatch N into n groups of size G, and channels C into F groups of size c.\n",
        "        y = y - y.mean(dim=0)               # [GnFcHW] Subtract mean over group.\n",
        "        y = y.square().mean(dim=0)          # [nFcHW]  Calc variance over group.\n",
        "        y = (y + 1e-8).sqrt()               # [nFcHW]  Calc stddev over group.\n",
        "        y = y.mean(dim=[2,3,4])             # [nF]     Take average over channels and pixels.\n",
        "        y = y.reshape(-1, F, 1, 1)          # [nF11]   Add missing dimensions.\n",
        "        y = y.repeat(G, 1, H, W)            # [NFHW]   Replicate over group and pixels.\n",
        "        x = torch.cat([x, y], dim=1)        # [NCHW]   Append to input as new channels.\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class DiscriminatorEpilogue(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels,                    # Number of input channels.\n",
        "        cmap_dim,                       # Dimensionality of mapped conditioning label, 0 = no label.\n",
        "        resolution,                     # Resolution of this block.\n",
        "        img_channels,                   # Number of input color channels.\n",
        "        architecture        = 'resnet', # Architecture: 'orig', 'skip', 'resnet'.\n",
        "        mbstd_group_size    = 4,        # Group size for the minibatch standard deviation layer, None = entire minibatch.\n",
        "        mbstd_num_channels  = 1,        # Number of features for the minibatch standard deviation layer, 0 = disable.\n",
        "        activation          = 'lrelu',  # Activation function: 'relu', 'lrelu', etc.\n",
        "        conv_clamp          = None,     # Clamp the output of convolution layers to +-X, None = disable clamping.\n",
        "    ):\n",
        "        assert architecture in ['orig', 'skip', 'resnet']\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.cmap_dim = cmap_dim\n",
        "        self.resolution = resolution\n",
        "        self.img_channels = img_channels\n",
        "        self.architecture = architecture\n",
        "\n",
        "        if architecture == 'skip':\n",
        "            self.fromrgb = Conv2dLayer(img_channels, in_channels, kernel_size=1, activation=activation)\n",
        "        self.mbstd = MinibatchStdLayer(group_size=mbstd_group_size, num_channels=mbstd_num_channels) if mbstd_num_channels > 0 else None\n",
        "        self.conv = Conv2dLayer(in_channels + mbstd_num_channels, in_channels, kernel_size=3, activation=activation, conv_clamp=conv_clamp)\n",
        "        self.fc = FullyConnectedLayer(in_channels * (resolution ** 2), in_channels, activation=activation)\n",
        "        self.out = FullyConnectedLayer(in_channels, 1 if cmap_dim == 0 else cmap_dim)\n",
        "\n",
        "    def forward(self, x, img, cmap, force_fp32=False):\n",
        "        misc.assert_shape(x, [None, self.in_channels, self.resolution, self.resolution]) # [NCHW]\n",
        "        _ = force_fp32 # unused\n",
        "        dtype = torch.float32\n",
        "        memory_format = torch.contiguous_format\n",
        "\n",
        "        # FromRGB.\n",
        "        x = x.to(dtype=dtype, memory_format=memory_format)\n",
        "        if self.architecture == 'skip':\n",
        "            misc.assert_shape(img, [None, self.img_channels, self.resolution, self.resolution])\n",
        "            img = img.to(dtype=dtype, memory_format=memory_format)\n",
        "            x = x + self.fromrgb(img)\n",
        "\n",
        "        # Main layers.\n",
        "        if self.mbstd is not None:\n",
        "            x = self.mbstd(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.fc(x.flatten(1))\n",
        "        x = self.out(x)\n",
        "\n",
        "        # Conditioning.\n",
        "        if self.cmap_dim > 0:\n",
        "            misc.assert_shape(cmap, [None, self.cmap_dim])\n",
        "            x = (x * cmap).sum(dim=1, keepdim=True) * (1 / np.sqrt(self.cmap_dim))\n",
        "\n",
        "        assert x.dtype == dtype\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class Discriminator(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        c_dim,                          # Conditioning label (C) dimensionality.\n",
        "        img_resolution,                 # Input resolution.\n",
        "        img_channels,                   # Number of input color channels.\n",
        "        architecture        = 'resnet', # Architecture: 'orig', 'skip', 'resnet'.\n",
        "        channel_base        = 32768,    # Overall multiplier for the number of channels.\n",
        "        channel_max         = 512,      # Maximum number of channels in any layer.\n",
        "        num_fp16_res        = 0,        # Use FP16 for the N highest resolutions.\n",
        "        conv_clamp          = None,     # Clamp the output of convolution layers to +-X, None = disable clamping.\n",
        "        cmap_dim            = None,     # Dimensionality of mapped conditioning label, None = default.\n",
        "        block_kwargs        = {},       # Arguments for DiscriminatorBlock.\n",
        "        mapping_kwargs      = {},       # Arguments for MappingNetwork.\n",
        "        epilogue_kwargs     = {},       # Arguments for DiscriminatorEpilogue.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.c_dim = c_dim\n",
        "        self.img_resolution = img_resolution\n",
        "        self.img_resolution_log2 = int(np.log2(img_resolution))\n",
        "        self.img_channels = img_channels\n",
        "        self.block_resolutions = [2 ** i for i in range(self.img_resolution_log2, 2, -1)]\n",
        "        channels_dict = {res: min(channel_base // res, channel_max) for res in self.block_resolutions + [4]}\n",
        "        fp16_resolution = max(2 ** (self.img_resolution_log2 + 1 - num_fp16_res), 8)\n",
        "\n",
        "        if cmap_dim is None:\n",
        "            cmap_dim = channels_dict[4]\n",
        "        if c_dim == 0:\n",
        "            cmap_dim = 0\n",
        "\n",
        "        common_kwargs = dict(img_channels=img_channels, architecture=architecture, conv_clamp=conv_clamp)\n",
        "        cur_layer_idx = 0\n",
        "        for res in self.block_resolutions:\n",
        "            in_channels = channels_dict[res] if res < img_resolution else 0\n",
        "            tmp_channels = channels_dict[res]\n",
        "            out_channels = channels_dict[res // 2]\n",
        "            use_fp16 = (res >= fp16_resolution)\n",
        "            block = DiscriminatorBlock(in_channels, tmp_channels, out_channels, resolution=res,\n",
        "                first_layer_idx=cur_layer_idx, use_fp16=use_fp16, **block_kwargs, **common_kwargs)\n",
        "            setattr(self, f'b{res}', block)\n",
        "            cur_layer_idx += block.num_layers\n",
        "        if c_dim > 0:\n",
        "            self.mapping = MappingNetwork(z_dim=0, c_dim=c_dim, w_dim=cmap_dim, num_ws=None, w_avg_beta=None, **mapping_kwargs)\n",
        "        self.b4 = DiscriminatorEpilogue(channels_dict[4], cmap_dim=cmap_dim, resolution=4, **epilogue_kwargs, **common_kwargs)\n",
        "\n",
        "    def forward(self, img, c, **block_kwargs):\n",
        "        x = None\n",
        "        for res in self.block_resolutions:\n",
        "            block = getattr(self, f'b{res}')\n",
        "            x, img = block(x, img, **block_kwargs)\n",
        "\n",
        "        cmap = None\n",
        "        if self.c_dim > 0:\n",
        "            cmap = self.mapping(None, c)\n",
        "        x = self.b4(x, img, cmap)\n",
        "        return x\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1a540bd8cc8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmisc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpersistence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv2d_resample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR9xQ6Y6ieQT"
      },
      "source": [
        "# Download public StyleGAN 256x256 resolution model to `stylegan-256px.pt`\n",
        "# import os\n",
        "# if 'stylegan-256px.pt' not in os.listdir(os.getcwd()):\n",
        "#     !wget --load-cookies /tmp/cookies.txt \"https://drive.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?export=download&id=1QlXFPIOFzsJyjZ1AtfpnVhqW4Z0r8GLZ' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1QlXFPIOFzsJyjZ1AtfpnVhqW4Z0r8GLZ\" -O stylegan-256px.pt && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVngUse5ih8i",
        "outputId": "248a82fe-716a-453d-e297-b1662ac986bf"
      },
      "source": [
        "print(os.listdir(os.getcwd()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'drive', 'data', 'stylegan-256px.pt', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVQQVEcpjAhi"
      },
      "source": [
        "def init_ema(model):\n",
        "    '''\n",
        "    Function that initializes a static model to store exponential moving average.\n",
        "    '''\n",
        "    model.eval()\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def accumulate_ema_weights(model_ema, model_tgt, decay=0.999):\n",
        "    '''\n",
        "    Function for updating exponential moving average of weights.\n",
        "    '''\n",
        "    ema = dict(model_ema.named_parameters())\n",
        "    tgt = dict(model_tgt.named_parameters())\n",
        "\n",
        "    for p_ema, p_tgt in zip(ema.values(), tgt.values()):\n",
        "        p_ema.data.mul_(decay).add_(p_tgt.data, alpha=1-decay)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuaFGqH9jEU_"
      },
      "source": [
        "def sample_noise(batch_size, code_dim, device, p=0.9):\n",
        "    '''\n",
        "    Function that samples noise with mixing regularization with probability p.\n",
        "    '''\n",
        "    if random.random() < p:\n",
        "        z11, z12, z21, z22 = torch.randn(4, batch_size, code_dim, device=device).unbind(0)\n",
        "        z1 = [z11, z12]\n",
        "        z2 = [z21, z22]\n",
        "\n",
        "    else:\n",
        "        z1, z2 = torch.randn(2, batch_size, code_dim, device=device).unbind(0)\n",
        "\n",
        "    return z1, z2"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObcnBP9pjHXt"
      },
      "source": [
        "from torch.autograd import grad\n",
        "\n",
        "def gradient_penalty(inputs, outputs):\n",
        "    '''\n",
        "    Function that computes gradient penalty given inputs and outputs.\n",
        "    '''\n",
        "    g = grad(outputs=outputs.sum(), inputs=inputs, create_graph=True)[0]\n",
        "    gp = (g.flatten(1).norm(2, dim=1) ** 2).mean()\n",
        "    gp = 10 / 2 * gp\n",
        "    return gp"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIfzvzMfjKFO"
      },
      "source": [
        "def freeze_discriminator_layers(d):\n",
        "    '''\n",
        "    Function that freezes the first four discriminator layers.\n",
        "    '''\n",
        "    # Naming patterns taken from official code repo\n",
        "    ls = ['progression.{}'.format(8 - i) for i in range(3)] + ['linear']\n",
        "\n",
        "    for name, p in d.named_parameters():\n",
        "        if any(l in name for l in ls):\n",
        "            p.requires_grad = False\n",
        "\n",
        "def unfreeze_discriminator_layers(d):\n",
        "    '''\n",
        "    Function that unfreezes the discriminator layers.\n",
        "    '''\n",
        "    for p in d.parameters():\n",
        "        p.requires_grad = True"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhleYWI8jRos"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "import re\n",
        "\n",
        "# Some training parameters\n",
        "finetune_steps = 500\n",
        "display_step = 50\n",
        "\n",
        "resolution = 256\n",
        "step = int(math.log2(resolution / 4))\n",
        "lr = 0.002\n",
        "betas = (0.0, 0.99)\n",
        "\n",
        "def finetune(generators, dis, dataloader, code_dim, device):\n",
        "    gen_ema, gen = generators\n",
        "    \n",
        "    gen_optim = torch.optim.Adam([\n",
        "        {\n",
        "            'params': gen.generator.parameters(),\n",
        "            'sched': 1.0,\n",
        "        },\n",
        "        {\n",
        "            'params': gen.style.parameters(),\n",
        "            'sched': 0.01,\n",
        "        },\n",
        "    ], betas=betas)\n",
        "    dis_optim = torch.optim.Adam(\n",
        "        [p for p in dis.parameters() if p.requires_grad],\n",
        "        lr=lr, betas=betas,\n",
        "    )\n",
        "\n",
        "    cur_step = 0\n",
        "    mean_gen_loss, mean_dis_loss = 0.0, 0.0\n",
        "\n",
        "    while cur_step < finetune_steps:\n",
        "        for x in tqdm(dataloader):\n",
        "            with torch.cuda.amp.autocast((device=='cuda')):\n",
        "                # Prep inputs\n",
        "                x = x.to(device)\n",
        "                z1, z2 = sample_noise(x.size(0), code_dim, device)\n",
        "\n",
        "                dis.zero_grad()\n",
        "                gen.zero_grad()\n",
        "\n",
        "                # Forward pass through generator\n",
        "                x_fake1 = gen(z1, step=step, alpha=1)\n",
        "\n",
        "                # Unfreeze discriminator\n",
        "                unfreeze_discriminator_layers(dis)\n",
        "\n",
        "                # Update discriminator\n",
        "                x.requires_grad = True\n",
        "                fake_pred = dis(x_fake1.detach(), step=step, alpha=1)\n",
        "                real_pred = dis(x, step=step, alpha=1)\n",
        "                real_gp = gradient_penalty(x, real_pred)\n",
        "\n",
        "                dis_loss = real_gp + F.softplus(-real_pred).mean() + F.softplus(fake_pred).mean()\n",
        "                mean_dis_loss += dis_loss.item() / display_step\n",
        "                dis_optim.zero_grad()\n",
        "                dis_loss.backward()\n",
        "                dis_optim.step()\n",
        "\n",
        "                # Freeze discriminator\n",
        "                freeze_discriminator_layers(dis)\n",
        "\n",
        "                # Update generator\n",
        "                x_fake2 = gen(z2, step=step, alpha=1)\n",
        "                fake_pred = dis(x_fake2, step=step, alpha=1)\n",
        "\n",
        "                gen_loss = F.softplus(-fake_pred).mean()\n",
        "                mean_gen_loss += gen_loss.item() / display_step\n",
        "                gen_optim.zero_grad()\n",
        "                gen_loss.backward()\n",
        "                gen_optim.step()\n",
        "\n",
        "                # Update EMA\n",
        "                accumulate_ema_weights(gen_ema, gen, decay=0.999)\n",
        "\n",
        "            # Schedule learning rate\n",
        "            for param_group in gen_optim.param_groups:\n",
        "                param_group['lr'] *= param_group['sched']\n",
        "\n",
        "            cur_step += 1\n",
        "            if cur_step % display_step == 0:\n",
        "                show_tensor_images(x_fake1.to(x.dtype))\n",
        "                show_tensor_images(x_fake2.to(x.dtype))\n",
        "                show_tensor_images(x)\n",
        "\n",
        "                print('Step {}. G loss: {:.5f}. \\t D loss: {:.5f}.'.format(cur_step, mean_gen_loss, mean_dis_loss))\n",
        "                mean_gen_loss = 0.0\n",
        "                mean_dis_loss = 0.0\n",
        "\n",
        "                # Delete previous checkpoint to reduce disk memory\n",
        "                if cur_step - display_step > 0:\n",
        "                    os.remove('stylegan-step={}.pt'.format(cur_step - display_step))\n",
        "                torch.save({\n",
        "                    'generator': gen.state_dict(),\n",
        "                    'g_running': gen_ema.state_dict(),\n",
        "                    'discriminator': dis.state_dict(),\n",
        "                    'g_optim': gen_optim.state_dict(),\n",
        "                    'd_optim': dis_optim.state_dict(),\n",
        "                    'step': cur_step,\n",
        "                }, 'stylegan-step={}.pt'.format(cur_step))\n",
        "\n",
        "            # End training if reached enough steps\n",
        "            if cur_step == finetune_steps:\n",
        "                break\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "86NDZSk0jV8L",
        "outputId": "1bcac868-311b-44cd-d29b-c1b28af7f282"
      },
      "source": [
        "code_dim = 512\n",
        "n_classes = 10\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "checkpoint = torch.load('/content/drive/MyDrive/sgData/afhqwild.pkl')\n",
        "\n",
        "gen = StyledGenerator(code_dim=code_dim).to(device)\n",
        "gen.load_state_dict(checkpoint['generator'])\n",
        "gen_ema = StyledGenerator(code_dim=code_dim).to(device)\n",
        "init_ema(gen_ema)\n",
        "gen_ema.load_state_dict(checkpoint['g_running'])\n",
        "\n",
        "dis = Discriminator(from_rgb_activate=True).to(device)\n",
        "dis.load_state_dict(checkpoint['discriminator'])\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    Dataset('/content/data', n_classes=n_classes),\n",
        "    batch_size=8, pin_memory=True, shuffle=True, drop_last=True,\n",
        ")\n",
        "\n",
        "finetune(\n",
        "    [gen_ema, gen],\n",
        "    dis,\n",
        "    dataloader,\n",
        "    code_dim,\n",
        "    device,\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6564f2c291c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/sgData/afhqwild.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStyledGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcode_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    775\u001b[0m             \"functionality.\")\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dnnlib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}